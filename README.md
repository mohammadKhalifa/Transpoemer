# Transpoemer
## Poetry generation using Transformer-based architectures 

This repo holds my experiments on using pre-trained Transformer-based architectures for Poetry generation. All of the experiments are done on *Arabic* Poetry.

# [Poetry Generation]



# [BERT] (https://github.com/google-research/bert)
BERT's original uses do not include language generation. Actually, its Masked Language Modelling objective makes it very difficult to sample from it. Anyway, I thought it would be a worthy experiment to generate 



# [GPT-2] 
TODO

