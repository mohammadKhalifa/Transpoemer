{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading The Pre-trained BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../../XLM/dumped/xlm_ar/u7t8spazn5/checkpoint.pth'\n",
    "reloaded = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to convert sentenes into BPE format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-158335c8d38b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Below is one way to bpe-ize sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'codes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfastbpe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tools/fastBPE/fast'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mto_bpe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'params' is not defined"
     ]
    }
   ],
   "source": [
    "# Below is one way to bpe-ize sentences\n",
    "codes = os.path.join(params.data_path, 'codes')\n",
    "fastbpe = os.path.join(os.getcwd(), 'tools/fastBPE/fast')\n",
    "\n",
    "def to_bpe(sentences):\n",
    "    # write sentences to tmp file\n",
    "    with open('/tmp/sentences', 'w') as fwrite:\n",
    "        for sent in sentences:\n",
    "            fwrite.write(sent + '\\n')\n",
    "    \n",
    "    # apply bpe to tmp file\n",
    "    os.system('%s applybpe /tmp/sentences.bpe /tmp/sentences %s' % (fastbpe, codes))\n",
    "    \n",
    "    # load bpe-ized sentences\n",
    "    sentences_bpe = []\n",
    "    with open('/tmp/sentences.bpe') as f:\n",
    "        for line in f:\n",
    "            sentences_bpe.append(line.rstrip())\n",
    "    \n",
    "    return sentences_bpe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNAttnDecoder(nn.Module):\n",
    "    \n",
    "    '''\n",
    "    RNN decoder class for generating output from BERT\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, embeddings_dim, vocab_size, hid_dim, n_layers, bert_dim, dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(embedding_dim=embeddings_dim, num_embeddings=vocab_size)# initialize decoder embeddings with the pretrained embeddings\n",
    "        self.rnn = nn.GRU(embeddings_dim + bert_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        self.out = nn.Linear(hid_dim, vocab_size)\n",
    "        \n",
    "        self.attn = nn.Linear(bert_dim + hid_dim , hid_dim)\n",
    "        self.v = nn.Parameter(torch.rand(hid_dim))\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        self.attn_softmax = torch.nn.Softmax(dim=1)\n",
    "        self.out_softmax = torch.nn.LogSoftmax(dim=2)\n",
    "\n",
    "    \n",
    "    def forward(self, x, hidden, encoder_outputs):\n",
    "        \n",
    "        '''\n",
    "        Decode only one timestep\n",
    "        \n",
    "        x is (batch_size)\n",
    "        hidden is (n_layers, batch_size, hid_dim)\n",
    "        \n",
    "        encoder_outputs is (S, B, D)\n",
    "        '''\n",
    "        assert x.size(0) == hidden.size(1)\n",
    "        \n",
    "        \n",
    "        # Attention\n",
    "        bs = hidden.size(1)\n",
    "        src_len= encoder_outputs.size(0)\n",
    "        \n",
    "        last_hidden = hidden[-1,:,:] # (B x D) use only the hidden state of the last layer.\n",
    "        \n",
    "        hidden_repeated = last_hidden.unsqueeze(0).repeat(src_len, 1, 1)\n",
    "        \n",
    "        # compute attention weights\n",
    "        attn_prod = self.attn(torch.cat([hidden_repeated, encoder_outputs], dim=2)) # S x B x H\n",
    "        attn_energy = self.tanh(attn_prod).permute([1,0,2]).contiguous() # B x S x H\n",
    "        \n",
    "        v= self.v.view(1, -1, 1).repeat(bs, 1, 1) # B x D x 1\n",
    "        attn_weights = self.attn_softmax(torch.bmm(attn_energy, v)).permute([0, 2, 1]) # B x 1 x S\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute([1, 0, 2]) # B x S x D\n",
    "        context_vector = torch.bmm(attn_weights, encoder_outputs).squeeze(1).unsqueeze(0) #1 x B x D\n",
    "        \n",
    "        \n",
    "        # rnn input\n",
    "        x = x.unsqueeze(0) # since sequence length is one (only one timestep)\n",
    "        x_emb = self.embedding(x) #1 x B x D\n",
    "        \n",
    "        rnn_input = torch.cat([x_emb, context_vector], dim=2)\n",
    "        output, hidden = self.rnn(rnn_input, hidden) # 1 x B x H\n",
    "        \n",
    "        prediction = self.out_softmax(self.out(output)) # 1 x B x V\n",
    "        \n",
    "        return prediction, hidden\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNDecoder(nn.Module):\n",
    "    \n",
    "    '''\n",
    "    RNN decoder class for generating output from BERT\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, embeddings_dim, vocab_size, hid_dim, n_layers, bert_dim, dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.output_dim = vocab_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(embedding_dim=embeddings_dim, num_embeddings=vocab_size)# initialize decoder embeddings with the pretrained embeddings\n",
    "        self.rnn = nn.GRU(embeddings_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        self.out = nn.Linear(hid_dim, vocab_size)\n",
    "        self.out_softmax = torch.nn.LogSoftmax(dim=2)\n",
    "\n",
    "    \n",
    "    def forward(self, x, hidden, encoder_outputs):\n",
    "        \n",
    "        '''\n",
    "        Decode only one timestep\n",
    "        \n",
    "        x is (batch_size)\n",
    "        hidden is (n_layers, batch_size, hid_dim)\n",
    "        \n",
    "        encoder_outputs is (S, B, D)\n",
    "        '''\n",
    "        assert x.size(0) == hidden.size(1)\n",
    "        \n",
    "        # Attention\n",
    "        bs = hidden.size(1)\n",
    "        src_len= encoder_outputs.size(0)\n",
    "\n",
    "        # rnn input\n",
    "        x = x.unsqueeze(0) # since sequence length is one (only one timestep)\n",
    "        x_emb = self.embedding(x) #1 x B x D\n",
    "        \n",
    "        output, hidden = self.rnn(x_emb, hidden) # 1 x B x H\n",
    "        prediction = self.out(output) # 1 x B x V\n",
    "        \n",
    "        \n",
    "        return prediction, hidden\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transpoemer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "class Transpoemer(nn.Module):\n",
    "    \n",
    "    def __init__(self, bert, params):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = bert\n",
    "        self.params = params\n",
    "        self.decoder = RNNAttnDecoder(300, vocab_size=params.n_words,\n",
    "                                  hid_dim=params.decoder_hidden, n_layers=params.decoder_n_layers,\n",
    "                                  bert_dim= bert.dim,\n",
    "                                 dropout=params.dropout)\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index = self.params.pad_index)\n",
    "        self.bert_to_decoder= nn.Linear(bert.dim, params.decoder_hidden*params.decoder_n_layers)\n",
    "        \n",
    "        self.tanh =nn.Tanh()\n",
    "    def forward(self, x, x_lengths, y=None, max_generation_length=20):\n",
    "\n",
    "        '''\n",
    "        This should autoregreesively generate the next token \n",
    "        \n",
    "        x is the input sequence [SrcSeqLen x B]\n",
    "        y is [TrgSeqLen x B] and is used only during finetuning\n",
    "        \n",
    "        if labels are not None, we return logits and cross entropy loss,\n",
    "        otherwise we justreturns logits tensor of size []\n",
    "        \n",
    "        returned logits is [TrgSeqxLen x B x V]\n",
    "        \n",
    "        '''\n",
    "    \n",
    "        bs = x.size(1)\n",
    "    \n",
    "        # get bert embeddings\n",
    "        bert_embeddings = self.bert('fwd', x=x, lengths=x_lengths, causal=True).contiguous()\n",
    "        # use embeddings max pooling as init hidden of the decoder\n",
    "        #bert_embeddings_maxpool, _ = bert_embeddings.max(dim=0) # B x D\n",
    "        bert_embeddings_meanpool = bert_embeddings.mean(dim=0)\n",
    "        \n",
    "        decoder_hidden =self.tanh(self.bert_to_decoder(bert_embeddings_meanpool)) # B x LH\n",
    "        decoder_hidden = decoder_hidden.view(self.params.decoder_n_layers, \n",
    "                                                       bert_embeddings_meanpool.size(0), -1).contiguous() #(L x B x Decoder_H)\n",
    "        \n",
    "        # autoregressive decoding \n",
    "        \n",
    "        all_outputs= []\n",
    "        \n",
    "        if y is not None: # training case\n",
    "\n",
    "            decoder_input = y[0,:] # <s> symbol\n",
    "            y = y[1:, :] # skip start symbol to avoid repeating it\n",
    "            \n",
    "            for t in range(y.shape[0]):\n",
    "                output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, bert_embeddings)\n",
    "                all_outputs.append(output)\n",
    "                decoder_input= y[t,:]\n",
    "        \n",
    "            #print(len(all_outputs))\n",
    "        \n",
    "        else : # greedy decoding for generation \n",
    "            \n",
    "            decoder_input = torch.LongTensor([dico.word2id['<s>']] * bs).to(\"cuda\") ## TODO: use the right device\n",
    "            for i in range(max_generation_length):\n",
    "                # (B)\n",
    "                output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, bert_embeddings) # 1 x B x V\n",
    "                \n",
    "                _, topi =  output.max(dim=2) # use predictions as input again # 1 x B \n",
    "                all_outputs.append(topi) \n",
    "                decoder_input = topi.squeeze(0) # for next step\n",
    "                        \n",
    "        all_outputs= torch.cat(all_outputs, dim=0) # trgSeqLen x B x V\n",
    "    \n",
    "        # computing loss if any\n",
    "        loss = None\n",
    "        \n",
    "        if y is not None:\n",
    "            loss = self.criterion(all_outputs.view(-1, all_outputs.size(-1)), y.view(-1))     \n",
    "        \n",
    "        return all_outputs, loss\n",
    "    \n",
    "    \n",
    "    def generate_next_verse(self, verse):\n",
    "    \n",
    "        verse_bpe = to_bpe([verse])\n",
    "        ids, lengths= shape_sentences(verse_bpe)\n",
    "\n",
    "        generated_word_ids, _ = self.forward(ids.to(\"cuda\"), lengths.to(\"cuda\"))\n",
    "        generated_word_ids = generated_word_ids.squeeze(1).cpu().numpy()\n",
    "        words = [dico.id2word[id] for id in generated_word_ids]\n",
    "        \n",
    "        words = [w for w in words if w not in ['<s>', '</s>', '<unk>']]\n",
    "        sent = ' '.join(words)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_verse(verse):\n",
    "\n",
    "    verse = '<s> %s </s>' %(verse)\n",
    "    verse_bpe = to_bpe([verse])\n",
    "    \n",
    "    ids, lengths= shape_sentences(verse_bpe)\n",
    "    generated_word_ids, _ = transpoemer(ids.to(\"cuda\"), lengths.to(\"cuda\"))\n",
    "    print(generated_word_ids.size())\n",
    "    generated_word_ids = generated_word_ids.squeeze(1).cpu().numpy()\n",
    "    words = [dico.id2word[id] for id in generated_word_ids]\n",
    "\n",
    "    words = [w for w in words if w not in ['<s>', '</s>', '<unk>']]\n",
    "    sent = ' '.join(words)\n",
    "    return sent\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_sentences(sentences):\n",
    "    bs = len(sentences)\n",
    "    slen = max([len(sent) for sent in sentences])\n",
    "\n",
    "    word_ids = torch.LongTensor(slen, bs).fill_(params.pad_index).contiguous()\n",
    "    for i in range(len(sentences)):\n",
    "        sent = torch.LongTensor([dico.index(w) for w in sentences[i]]).contiguous()\n",
    "        word_ids[:len(sent), i] = sent\n",
    "\n",
    "    lengths = torch.LongTensor([len(sent) for sent in sentences])\n",
    "    \n",
    "    return word_ids, lengths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poetry Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class PoetryDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, poetry_file, max_len=50):\n",
    "        \n",
    "        self.poetry_file=poetry_file \n",
    "        verses=[]\n",
    "        cnt=0\n",
    "        with open(poetry_file, 'r', encoding='utf-8') as f:\n",
    "            for verse in f:\n",
    "                verses.append(verse.strip())\n",
    "                cnt+=1\n",
    "        \n",
    "        print(\"BPEing verses...\")\n",
    "        \n",
    "        verses= to_bpe(verses)\n",
    "        print(\"Done.\")\n",
    "        tokenized_verses=[]\n",
    "        for verse in verses:\n",
    "            if not verse.strip(): # skip empty verses\n",
    "                continue\n",
    "            if verse.strip() == '#': # end of poem\n",
    "                tokenized_verses.append('<s> <special0> </s>'.split()) # add poem separator\n",
    "            else:\n",
    "                tokens = verse.strip().split()[:max_len - 2]\n",
    "                tokens = ['<s>'] + tokens + ['</s>']\n",
    "                tokenized_verses.append(tokens)\n",
    "        \n",
    "        tokenized_verses = np.array(tokenized_verses)\n",
    "        # creating X, Y\n",
    "        X = []\n",
    "        Y = []\n",
    "        \n",
    "        even_length = (len(tokenized_verses) //2) * 2\n",
    "        tokenized_verses= tokenized_verses[:even_length]\n",
    "        \n",
    "        for i in range(even_length - 1):\n",
    "            X.append(tokenized_verses[i])\n",
    "            Y.append(tokenized_verses[i+1])\n",
    "            \n",
    "        assert len(X) == len(Y)\n",
    "        \n",
    "        self.X, self.X_len = shape_sentences(X) # Len x N\n",
    "        self.Y, self.Y_len = shape_sentences(Y)\n",
    "        \n",
    "        print(self.X.shape)\n",
    "    def __len__(self):\n",
    "        return self.X.shape[1]\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        \n",
    "        return self.X[:, idx], self.X_len[idx], self.Y[:, idx], self.Y_len[idx]\n",
    "        #ids, lens = shape_sentences(tokens)\n",
    "        #ids = ids.T.contiguous()\n",
    "        #print(ids.size())\n",
    "        #return ids.T, lens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPEing verses...\n",
      "Done.\n",
      "torch.Size([50, 590001])\n"
     ]
    }
   ],
   "source": [
    "dataset=PoetryDataset('poems_separated_verses.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.metrics import RunningAverage, Accuracy\n",
    "from ignite.handlers import ModelCheckpoint\n",
    "from ignite.contrib.handlers import CosineAnnealingScheduler, PiecewiseLinear, create_lr_scheduler_with_warmup, ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "Config = namedtuple('Config',\n",
    "  field_names=\"batch_size, n_epochs, lr, gradient_accumulation_steps, n_warmup, max_norm, dropout, log_dir, device\")\n",
    "\n",
    "args = Config(32, 50, 5e-4, 1, 1, 10.0, 0.1, \"./poetry_models\", \"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training function\n",
    "\n",
    "\n",
    "def update(engine, batch):\n",
    "    #transpoemer.train()\n",
    "    \n",
    "    src_ids, src_lengths, trg_ids, trg_length = batch\n",
    "      \n",
    "    src_ids = src_ids.T.contiguous().to(args.device)\n",
    "    src_lengths = src_lengths.to(args.device)\n",
    "    \n",
    "    trg_ids = trg_ids.T.contiguous().to(args.device)\n",
    "    trg_length = trg_length.to(args.device)\n",
    "        \n",
    "    logits, loss = transpoemer(x=src_ids, x_lengths=src_lengths,y=trg_ids)\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if engine.state.iteration % 1000 == 0:\n",
    "        print(generate_next_verse('عيناك غابتا نخيل ساعة السحر'))\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "trainer = Engine(update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/add/anaconda3/envs/py35/lib/python3.6/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "# dataset loader\n",
    "train_dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n",
    "# optimizer\n",
    "# model\n",
    "\n",
    "params['decoder_hidden']= 1024\n",
    "params['decoder_n_layers'] = 1\n",
    "\n",
    "transpoemer = Transpoemer(model, params).to(\"cuda\")\n",
    "optimizer = torch.optim.Adam(transpoemer.parameters(), lr=args.lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze bert weights\n",
    "transpoemer.bert.eval()\n",
    "for name, p in transpoemer.named_parameters():\n",
    "    if 'bert.' in name and not p.requires_grad:\n",
    "        print(name)\n",
    "        p.requires_grad=True\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunningAverage(output_transform=lambda x: x).attach(trainer, \"loss\")\n",
    "ProgressBar(persist=True).attach(trainer, metric_names=['loss'])\n",
    "\n",
    "\n",
    "# Learning rate schedule: linearly warm-up to lr and then decrease the learning rate to zero with cosine\n",
    "#cos_scheduler = CosineAnnealingScheduler(optimizer, 'lr', args.lr, 0.0, len(train_dataloader) * args.n_epochs)\n",
    "#scheduler = create_lr_scheduler_with_warmup(cos_scheduler, 0.0, args.lr, args.n_warmup)\n",
    "#trainer.add_event_handler(Events.ITERATION_STARTED, cos_scheduler)\n",
    "\n",
    "# Save checkpoints and training config\n",
    "checkpoint_handler = ModelCheckpoint(args.log_dir, 'checkpoint', save_interval=1, n_saved=1, require_empty=False)\n",
    "trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpoint_handler, {'attn-1x1024': model})\n",
    "torch.save(args, os.path.join(args.log_dir, 'training_args.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b0718a0acd44ec4b0912691bb15d104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18438), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 1])\n",
      "ن رسم ن رسم ن رسم ن رسم\n",
      "torch.Size([20, 1])\n",
      "ن ن ن ن غ@@ لقت ن غ@@ لقت ن غ@@\n",
      "torch.Size([20, 1])\n",
      "و ن ن ن ن ن ن ن ن ن\n",
      "torch.Size([20, 1])\n",
      "ن ن ن ن ن ن ن ن ن ن\n",
      "torch.Size([20, 1])\n",
      "و ن ن ن ن ن ن ن ن ن\n",
      "torch.Size([20, 1])\n",
      "ن ن ن ن ن ن ن ن ن ن\n",
      "torch.Size([20, 1])\n",
      "ف@@ نا نت نت نت نت نت ي ي ي\n",
      "torch.Size([20, 1])\n",
      "نا ضد العلاقة نا ضد العلاقة و مش@@ ي خ@@ رى\n",
      "torch.Size([20, 1])\n",
      "\n",
      "torch.Size([20, 1])\n",
      "و@@ نت يا نت يا نت يا نت يا نت يا نت يا\n",
      "torch.Size([20, 1])\n",
      "\n",
      "torch.Size([20, 1])\n",
      "و@@ ن ن رى ن رى ن رى ن رى\n",
      "torch.Size([20, 1])\n",
      "نا لا رى سببا س@@ لتي خ@@ رى ذا ما رى ال@@\n",
      "torch.Size([20, 1])\n",
      "و@@ رى ن غ@@ دو على ص@@ بوة من جل ن كون غ@@\n",
      "torch.Size([20, 1])\n",
      "م ن ح@@ لـ@@ ت ن ر@@ يت ح@@ لام من ي\n",
      "torch.Size([20, 1])\n",
      "ن حرر في ي شيء ي@@ اسي@@ دة نا من جل ن قول\n",
      "torch.Size([20, 1])\n",
      "و@@ نا رى ن ح@@ بك ن ح@@ بك ن ح@@ بك\n",
      "torch.Size([20, 1])\n",
      "و ح@@ بك و ح@@ بك و ح@@ بك و ح@@ لامي و\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35fa7023f0194cbcbeb2010a8eb5a061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18438), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e67495a5ca949aabc83573fb84d0a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18438), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-6d50541d1801>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.6/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Engine run is terminating due to exception: %s.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.6/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.6/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs)\u001b[0m\n\u001b[1;32m    344\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m                 \u001b[0mhours\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch[%s] Complete. Time taken: %02d:%02d:%02d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhours\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_terminate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.6/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Current run is terminating due to exception: %s.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.6/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.6/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    303\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mITERATION_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mITERATION_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_terminate\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_terminate_single_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-e7e40d81e4e3>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(engine, batch)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranspoemer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_lengths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_lengths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrg_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.run(train_dataloader, max_epochs=args.n_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
